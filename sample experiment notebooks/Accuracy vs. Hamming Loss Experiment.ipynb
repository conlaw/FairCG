{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy vs. Hamming Loss Trials\n",
    "\n",
    "This notebook goes over sample code for testing the difference between optimizing for accuracy vs hamming loss. Note this notebook used pre-mined rules (included in the rules folder), and our pre-split and binerized data (included in data_split folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from master_model import *\n",
    "from DNFRuleModel import DNFRuleModel\n",
    "from scipy.stats import bernoulli\n",
    "from fairness_modules import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for computing fairness/accuracy\n",
    "def compute_TPR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TPR'] = sum(preds[Y])/len(preds[Y])\n",
    "    res['TPR_1'] = sum(preds[Y & group])/len(preds[Y & group])\n",
    "    res['TPR_2'] = sum(preds[Y & ~group])/len(preds[Y & ~group])\n",
    "    res['TPR_GAP'] = abs(res['TPR_1'] - res['TPR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_TNR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TNR'] = sum(~preds[~Y])/len(preds[~Y])\n",
    "    res['TNR_1'] = sum(~preds[~Y & group])/len(preds[~Y & group])\n",
    "    res['TNR_2'] = sum(~preds[~Y & ~group])/len(preds[~Y & ~group])\n",
    "    res['TNR_GAP'] = abs(res['TNR_1'] - res['TNR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_ACC_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['ACC'] = sum(preds == Y)/len(Y)\n",
    "    res['ACC_1'] = sum(preds[group] == Y[group])/len(Y[group])\n",
    "    res['ACC_2'] = sum(preds[~group] == Y[~group])/len(Y[~group])\n",
    "    res['ACC_GAP'] = abs(res['ACC_1'] - res['ACC_2'])\n",
    "    return res\n",
    "    \n",
    "def compute_EqOpp(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group)\n",
    "\n",
    "def compute_EqOd(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group).update(compute_TNR_GAP(preds, Y, group) )\n",
    "\n",
    "def compute_AccDisp(preds, Y, group):\n",
    "    return compute_ACC_GAP(preds, Y, group)\n",
    "\n",
    "def compute_fairness(preds,Y,group):\n",
    "    res = compute_TPR_GAP(preds, Y, group)\n",
    "    res.update(compute_TNR_GAP(preds, Y, group))\n",
    "    res.update(compute_ACC_GAP(preds,Y,group))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpre function for evaluating diffrent IP Models\n",
    "def eval_model(model, ruleMod, rules, X_tr, Y_tr, X_tst, Y_tst):\n",
    "    #Initialize model with rule sest\n",
    "    model.addRule(rules)\n",
    "    \n",
    "    #Solve the model\n",
    "    start = time.perf_counter()\n",
    "    results = model.solve(verbose = True, relax = False)\n",
    "    end = time.perf_counter() - start\n",
    "    \n",
    "    #Extract predictions for tesst and train data\n",
    "    fitRules = results['ruleSet']\n",
    "    \n",
    "    if len(fitRules) > 0:\n",
    "        preds_tr = ruleMod.predict(X_tr, fitRules)\n",
    "        preds = ruleMod.predict(X_tst, fitRules)\n",
    "    else:\n",
    "        preds_tr = (np.zeros(Y_tr.shape)).astype(np.bool)\n",
    "        preds = (np.zeros(Y_tst.shape)).astype(np.bool)\n",
    "    \n",
    "    #Compute metrics on performance of final rule setss\n",
    "    tr_acc = np.mean(preds_tr == Y_tr)\n",
    "    tst_acc = np.mean(preds == Y_tst)\n",
    "    complexity = len(fitRules) + np.sum(fitRules)\n",
    "    \n",
    "    res = {'time': end,\n",
    "           'tr_acc': tr_acc,\n",
    "           'tst_acc': tst_acc,\n",
    "           'complexity': complexity,\n",
    "           'obj': results['obj']\n",
    "          }\n",
    "    return res, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Run Experiment\n",
    "'''\n",
    "results = []\n",
    "\n",
    "#Model Parameters\n",
    "test_params = {\n",
    "            'price_limit': 45,\n",
    "            'train_limit': 300,\n",
    "            'fixed_model_params': {\n",
    "                'ruleGenerator': 'Hybrid',\n",
    "                'masterSolver':'barrierCrossover',\n",
    "                'numRulesToReturn': 100,\n",
    "                'fairness_module': 'EqOfOp',\n",
    "                'IP_time_limit': 600\n",
    "            },\n",
    "        }\n",
    "\n",
    "protected_features = {'compas': 'race', \n",
    "                      'adult': 'gender',\n",
    "                      'default': 'X2'\n",
    "                     }\n",
    "group_var = 'race'\n",
    "data_set = 'compas'\n",
    "\n",
    "#Loop over folds\n",
    "for i in range(10):\n",
    "    \n",
    "    #Extract data\n",
    "    print('***** FOLD %d ******'%i)\n",
    "    train  = pd.read_csv('data_split/bin_'+data_set+'_train_%d.csv'%i)\n",
    "    test = pd.read_csv('data_split/bin_'+data_set+'_test_%d.csv'%i)\n",
    "    X_tr = train.drop('Y',axis=1).to_numpy()\n",
    "    Y_tr = train['Y'].to_numpy()\n",
    "    X_tst = test.drop('Y',axis=1).to_numpy()\n",
    "    Y_tst = test['Y'].to_numpy()\n",
    "    \n",
    "    #Load in rule sets - for these experiments we use rules from rfs and our faircg process\n",
    "    isFirst = True\n",
    "    for ruleset in ['faircg','rf']:\n",
    "        print('***** Ruleset %s ******'%ruleset)\n",
    "        new_rules = np.load('rules/'+ruleset+'_%s_fold_%d.npy'%('compas', i)).astype(int)\n",
    "        if isFirst:\n",
    "            rules = new_rules\n",
    "            isFirst = False\n",
    "        else:\n",
    "            rules = np.concatenate([rules,new_rules])\n",
    "    \n",
    "    #For range of epsilon and complexities run the mdoels\n",
    "    for eps in [0.01, 0.05, 0.1, 0.2, 1]:\n",
    "        for C in [10, 20, 30]:\n",
    "            print('***** EPS %f ******'%eps)\n",
    "            \n",
    "            #Set model parameters\n",
    "            test_params = test_params.copy()\n",
    "            test_params['fixed_model_params']['epsilon'] = eps\n",
    "            test_params['fixed_model_params']['ruleComplexity'] = C\n",
    "            test_params['fixed_model_params']['group'] = train[group_var].to_numpy()\n",
    "            \n",
    "            #Create Rule + fairness Modules\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            fairMod = EqualityOfOpportunity.EqualityOfOpportunity(test_params['fixed_model_params'])\n",
    "            \n",
    "            #Evaluate Hamming Loss Master Model\n",
    "            res,preds = eval_model(CompactDoubleSidedMaster.CompactDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "\n",
    "            res['dataset'] = data_set\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'hamming loss'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "            \n",
    "            # Evaluate Acccuraacy master model\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            fairMod = EqualityOfOpportunity.EqualityOfOpportunity(test_params['fixed_model_params'])\n",
    "\n",
    "            res,preds = eval_model(ZeroOneDoubleSidedMaster.ZeroOneDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "            \n",
    "\n",
    "\n",
    "            res['dataset'] = 'compas'\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'accuracy'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "            \n",
    "results = pd.DataFrame.from_records(results)\n",
    "results.to_csv('01_vs_Hamming.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
