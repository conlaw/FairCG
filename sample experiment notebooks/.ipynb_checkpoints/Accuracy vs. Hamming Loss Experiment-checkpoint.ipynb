{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy vs. Hamming Loss Trials\n",
    "\n",
    "This notebook goes over sample code for testing the difference between optimizing for accuracy vs hamming loss. Note this notebook used pre-mined rules (included in the rules folder), and our pre-split and binerized data (included in split_data folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from master_model import *\n",
    "from DNFRuleModel import DNFRuleModel\n",
    "from scipy.stats import bernoulli\n",
    "from fairness_modules import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for computing fairness/accuracy\n",
    "def compute_TPR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TPR'] = sum(preds[Y])/len(preds[Y])\n",
    "    res['TPR_1'] = sum(preds[Y & group])/len(preds[Y & group])\n",
    "    res['TPR_2'] = sum(preds[Y & ~group])/len(preds[Y & ~group])\n",
    "    res['TPR_GAP'] = abs(res['TPR_1'] - res['TPR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_TNR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TNR'] = sum(~preds[~Y])/len(preds[~Y])\n",
    "    res['TNR_1'] = sum(~preds[~Y & group])/len(preds[~Y & group])\n",
    "    res['TNR_2'] = sum(~preds[~Y & ~group])/len(preds[~Y & ~group])\n",
    "    res['TNR_GAP'] = abs(res['TNR_1'] - res['TNR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_ACC_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['ACC'] = sum(preds == Y)/len(Y)\n",
    "    res['ACC_1'] = sum(preds[group] == Y[group])/len(Y[group])\n",
    "    res['ACC_2'] = sum(preds[~group] == Y[~group])/len(Y[~group])\n",
    "    res['ACC_GAP'] = abs(res['ACC_1'] - res['ACC_2'])\n",
    "    return res\n",
    "    \n",
    "def compute_EqOpp(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group)\n",
    "\n",
    "def compute_EqOd(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group).update(compute_TNR_GAP(preds, Y, group) )\n",
    "\n",
    "def compute_AccDisp(preds, Y, group):\n",
    "    return compute_ACC_GAP(preds, Y, group)\n",
    "\n",
    "def compute_fairness(preds,Y,group):\n",
    "    res = compute_TPR_GAP(preds, Y, group)\n",
    "    res.update(compute_TNR_GAP(preds, Y, group))\n",
    "    res.update(compute_ACC_GAP(preds,Y,group))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpre function for evaluating diffrent IP Models\n",
    "def eval_model(model, ruleMod, rules, X_tr, Y_tr, X_tst, Y_tst):\n",
    "    #Initialize model with rule sest\n",
    "    model.addRule(rules)\n",
    "    \n",
    "    #Solve the mod\n",
    "    start = time.perf_counter()\n",
    "    results = model.solve(verbose = True, relax = False)\n",
    "    end = time.perf_counter() - start\n",
    "    \n",
    "    fitRules = results['ruleSet']\n",
    "    \n",
    "    if len(fitRules) > 0:\n",
    "        preds_tr = ruleMod.predict(X_tr, fitRules)\n",
    "        preds = ruleMod.predict(X_tst, fitRules)\n",
    "    else:\n",
    "        preds_tr = (np.zeros(Y_tr.shape)).astype(np.bool)\n",
    "        preds = (np.zeros(Y_tst.shape)).astype(np.bool)\n",
    "        \n",
    "    tr_acc = np.mean(preds_tr == Y_tr)\n",
    "    tst_acc = np.mean(preds == Y_tst)\n",
    "    complexity = len(fitRules) + np.sum(fitRules)\n",
    "    \n",
    "    res = {'time': end,\n",
    "           'tr_acc': tr_acc,\n",
    "           'tst_acc': tst_acc,\n",
    "           'complexity': complexity,\n",
    "           'obj': results['obj']\n",
    "          }\n",
    "    return res, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "test_params = {\n",
    "            'price_limit': 45,\n",
    "            'train_limit': 300,\n",
    "            'fixed_model_params': {\n",
    "                'ruleGenerator': 'Hybrid',\n",
    "                'masterSolver':'barrierCrossover',\n",
    "                'numRulesToReturn': 100,\n",
    "                'fairness_module': 'EqOfOp',\n",
    "                'IP_time_limit': 600\n",
    "            },\n",
    "        }\n",
    "\n",
    "protected_features = {'compas': 'race', \n",
    "                      'adult': 'gender',\n",
    "                      'default': 'X2'\n",
    "                     }\n",
    "group_var = 'race'\n",
    "\n",
    "for i in range(6):\n",
    "    print('***** FOLD %d ******'%i)\n",
    "    train  = pd.read_csv('data_split/bin_'+'compas'+'_train_%d.csv'%i)\n",
    "    test = pd.read_csv('data_split/bin_'+'compas'+'_test_%d.csv'%i)\n",
    "    X_tr = train.drop('Y',axis=1).to_numpy()\n",
    "    Y_tr = train['Y'].to_numpy()\n",
    "    X_tst = test.drop('Y',axis=1).to_numpy()\n",
    "    Y_tst = test['Y'].to_numpy()\n",
    "    \n",
    "    isFirst = True\n",
    "    for ruleset in ['faircg','rf']:\n",
    "        print('***** Ruleset %s ******'%ruleset)\n",
    "        new_rules = np.load('rules/'+ruleset+'_%s_fold_%d.npy'%('compas', i)).astype(int)\n",
    "        if isFirst:\n",
    "            rules = new_rules\n",
    "            isFirst = False\n",
    "        else:\n",
    "            rules = np.concatenate([rules,new_rules])\n",
    "    \n",
    "    for eps in [0.01, 0.05, 0.1, 0.2, 1]:\n",
    "        for C in [30]:\n",
    "            print('***** EPS %f ******'%eps)\n",
    "\n",
    "            test_params = test_params.copy()\n",
    "            test_params['fixed_model_params']['epsilon'] = eps\n",
    "            test_params['fixed_model_params']['ruleComplexity'] = C\n",
    "            test_params['fixed_model_params']['group'] = train[group_var].to_numpy()\n",
    "\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            fairMod = EqualityOfOpportunity.EqualityOfOpportunity(test_params['fixed_model_params'])\n",
    "\n",
    "            res,preds = eval_model(CompactDoubleSidedMaster.CompactDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "\n",
    "            res['dataset'] = 'compas'\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'hamming loss'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            res,preds = eval_model(ZeroOneDoubleSidedMaster.ZeroOneDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "            \n",
    "\n",
    "\n",
    "            res['dataset'] = 'compas'\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'accuracy'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "            \n",
    "results = pd.DataFrame.from_records(results)\n",
    "results.to_csv('01_vs_Hamming_fair2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "test_params = {\n",
    "            'price_limit': 45,\n",
    "            'train_limit': 300,\n",
    "            'fixed_model_params': {\n",
    "                'ruleGenerator': 'Hybrid',\n",
    "                'masterSolver':'barrierCrossover',\n",
    "                'numRulesToReturn': 100,\n",
    "                'fairness_module': 'EqOfOp',\n",
    "                'IP_time_limit': 600\n",
    "            },\n",
    "        }\n",
    "\n",
    "protected_features = {'compas': 'race', \n",
    "                      'adult': 'gender',\n",
    "                      'default': 'X2'\n",
    "                     }\n",
    "group_var = 'race'\n",
    "\n",
    "for i in range(6):\n",
    "    print('***** FOLD %d ******'%i)\n",
    "    train  = pd.read_csv('data_split/bin_'+'compas'+'_train_%d.csv'%i)\n",
    "    test = pd.read_csv('data_split/bin_'+'compas'+'_test_%d.csv'%i)\n",
    "    X_tr = train.drop('Y',axis=1).to_numpy()\n",
    "    Y_tr = train['Y'].to_numpy()\n",
    "    X_tst = test.drop('Y',axis=1).to_numpy()\n",
    "    Y_tst = test['Y'].to_numpy()\n",
    "    \n",
    "    isFirst = True\n",
    "    for ruleset in ['faircg','rf']:\n",
    "        print('***** Ruleset %s ******'%ruleset)\n",
    "        new_rules = np.load('rules/'+ruleset+'_%s_fold_%d.npy'%('compas', i)).astype(int)\n",
    "        if isFirst:\n",
    "            rules = new_rules\n",
    "            isFirst = False\n",
    "        else:\n",
    "            rules = np.concatenate([rules,new_rules])\n",
    "    \n",
    "    for eps in [0.025]:\n",
    "        for C in [30]:\n",
    "            print('***** EPS %f ******'%eps)\n",
    "\n",
    "            test_params = test_params.copy()\n",
    "            test_params['fixed_model_params']['epsilon'] = eps\n",
    "            test_params['fixed_model_params']['ruleComplexity'] = C\n",
    "            test_params['fixed_model_params']['group'] = train[group_var].to_numpy()\n",
    "\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            fairMod = EqualityOfOpportunity.EqualityOfOpportunity(test_params['fixed_model_params'])\n",
    "\n",
    "            res,preds = eval_model(CompactDoubleSidedMaster.CompactDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "\n",
    "            res['dataset'] = 'compas'\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'hamming loss'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "\n",
    "            ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "            res,preds = eval_model(ZeroOneDoubleSidedMaster.ZeroOneDoubleSidedMaster(ruleMod, fairMod, \n",
    "                                                                                     test_params['fixed_model_params']),\n",
    "                                   ruleMod, \n",
    "                                   rules, \n",
    "                                   train.drop('Y',axis=1).to_numpy(), train['Y'].to_numpy(),\n",
    "                                   test.drop('Y',axis=1).to_numpy(), test['Y'].to_numpy())\n",
    "            \n",
    "\n",
    "\n",
    "            res['dataset'] = 'compas'\n",
    "            res['fold'] = i\n",
    "            res['eps'] = eps\n",
    "            res['fairMet'] = 'EqOp'\n",
    "            res['C'] = C\n",
    "            res['method'] = 'accuracy'\n",
    "            res.update(compute_fairness(preds,test['Y'],test[group_var]))\n",
    "            results.append(res)\n",
    "            \n",
    "results = pd.DataFrame.from_records(results)\n",
    "results.to_csv('01_vs_Hamming_fair_feb3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (OLD PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_results = (results.groupby(['method','eps']).agg({'tr_acc': 'mean', 'tst_acc':'mean',\n",
    "                                           'complexity': 'mean', 'time': 'mean'})\n",
    " .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_by_side = (agg_results.query('method == \\'accuracy\\'')\n",
    "                .merge(agg_results.query('method == \\'hamming loss\\''),\n",
    "                      on='eps')\n",
    "                .assign(acc_diff = lambda df: df.tr_acc_x - df.tr_acc_y,\n",
    "                        acc_diff_tst = lambda df: df.tst_acc_x - df.tst_acc_y,\n",
    "                       )\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = agg_results, x = 'eps', y='tr_acc', hue = 'method', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = agg_results, x = 'eps', y='tst_acc', hue = 'method', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = agg_results, x = 'eps', y='time', hue = 'method', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = side_by_side, x = 'eps', y='acc_diff', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = side_by_side, x = 'eps', y='acc_diff_tst', kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrality Gap Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model2(method, model, ruleMod, rules, X_tr, Y_tr, X_tst, Y_tst, relax):\n",
    "    model.addRule(rules)\n",
    "    start = time.perf_counter()\n",
    "    results = model.solve(verbose = True, relax = relax)\n",
    "    end = time.perf_counter() - start\n",
    "        \n",
    "    res = {'model': method,\n",
    "           'time': end,\n",
    "           'obj': results['obj']\n",
    "          }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 10\n",
    "n = 2000\n",
    "n_test = 500\n",
    "num_rounds = 50\n",
    "class_imbalances = np.linspace(0.01,0.99,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = generate_rules(num_features)\n",
    "results = []\n",
    "for imb in class_imbalances:\n",
    "    print('**** CLASS IMBALANCE %f ****'%imb)\n",
    "    for i in range(num_rounds):\n",
    "        train_data, test_data = generate_data(n, num_features, imb, n_test)\n",
    "        X_tr = train_data.drop('Y',axis=1).to_numpy()\n",
    "        Y_tr = train_data['Y'].to_numpy()\n",
    "        X_tst = test_data.drop('Y',axis=1).to_numpy()\n",
    "        Y_tst = test_data['Y'].to_numpy()\n",
    "\n",
    "        \n",
    "        ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "        fairMod = NoFair.NoFair({})\n",
    "        \n",
    "        res = eval_model2('LP', \n",
    "                   CompactOneSidedMaster.CompactOneSidedMaster(ruleMod, fairMod, {'ruleComplexity': 20}),\n",
    "                   ruleMod, \n",
    "                   rules, \n",
    "                   X_tr, Y_tr, X_tst, Y_tst, True)\n",
    "\n",
    "        res['imbalance'] = imb\n",
    "        res['trial'] = i\n",
    "\n",
    "        results.append(res)\n",
    "        \n",
    "        ruleMod = DNFRuleModel(X_tr, Y_tr)\n",
    "        res = eval_model2('IP', \n",
    "                   CompactOneSidedMaster.CompactOneSidedMaster(ruleMod, fairMod, {'ruleComplexity': 20}),\n",
    "                   ruleMod, \n",
    "                   rules, \n",
    "                   X_tr, Y_tr, X_tst, Y_tst, False)\n",
    "\n",
    "        res['imbalance'] = imb\n",
    "        res['trial'] = i\n",
    "        results.append(res)\n",
    "        \n",
    "results = pd.DataFrame.from_records(results)\n",
    "results.to_csv('integrality_gap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('integrality_gap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = (results.query('model == \\'LP\\'')\n",
    " .merge(results.query('model == \\'IP\\''), on=['trial','imbalance'])\n",
    " .assign(integrality_gap = lambda df: df.obj_y/df.obj_x)\n",
    ")\n",
    "\n",
    "imb_init = (gap\n",
    " .groupby('imbalance')\n",
    " ['integrality_gap']\n",
    " .mean()\n",
    " .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(gap['integrality_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data = imb_int, x = 'imbalance', y='integrality_gap',kind='line')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
