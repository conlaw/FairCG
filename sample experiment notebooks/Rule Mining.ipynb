{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Mining Notebook\n",
    "\n",
    "This is a sample notebook for our procedure to extract decision rules from tree based models including decision trees, random forests, and fair decision trees. Note this notebook requires boths scikit-learn and fairlearn, and is based around binerized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, TruePositiveRateParity, ErrorRateParity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for computing fairness/accuracy\n",
    "def compute_TPR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TPR'] = sum(preds[Y])/len(preds[Y])\n",
    "    res['TPR_1'] = sum(preds[Y & group])/len(preds[Y & group])\n",
    "    res['TPR_2'] = sum(preds[Y & ~group])/len(preds[Y & ~group])\n",
    "    res['TPR_GAP'] = abs(res['TPR_1'] - res['TPR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_TNR_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['TNR'] = sum(~preds[~Y])/len(preds[~Y])\n",
    "    res['TNR_1'] = sum(~preds[~Y & group])/len(preds[~Y & group])\n",
    "    res['TNR_2'] = sum(~preds[~Y & ~group])/len(preds[~Y & ~group])\n",
    "    res['TNR_GAP'] = abs(res['TNR_1'] - res['TNR_2'])\n",
    "    return res\n",
    "\n",
    "def compute_ACC_GAP(preds, Y, group):\n",
    "    res = {}\n",
    "    res['ACC'] = sum(preds == Y)/len(Y)\n",
    "    res['ACC_1'] = sum(preds[group] == Y[group])/len(Y[group])\n",
    "    res['ACC_2'] = sum(preds[~group] == Y[~group])/len(Y[~group])\n",
    "    res['ACC_GAP'] = abs(res['ACC_1'] - res['ACC_2'])\n",
    "    return res\n",
    "    \n",
    "def compute_EqOpp(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group)\n",
    "\n",
    "def compute_EqOd(preds, Y, group):\n",
    "    return compute_TPR_GAP(preds, Y, group).update(compute_TNR_GAP(preds, Y, group) )\n",
    "\n",
    "def compute_AccDisp(preds, Y, group):\n",
    "    return compute_ACC_GAP(preds, Y, group)\n",
    "\n",
    "def compute_fairness(preds,Y,group):\n",
    "    res = compute_TPR_GAP(preds, Y, group)\n",
    "    res.update(compute_TNR_GAP(preds, Y, group))\n",
    "    res.update(compute_ACC_GAP(preds,Y,group))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function rule to extract decision rules from a tree\n",
    "def extract_rules(tree, X, orig_features, subset, subset_c):\n",
    "    #Extract Relevant features about the tree\n",
    "    leaf_id = tree.apply(X)\n",
    "    node_indicator = tree.decision_path(X)\n",
    "    threshold = tree.tree_.threshold\n",
    "    feature = tree.tree_.feature\n",
    "    \n",
    "    #Get one sample for each leaf node\n",
    "    samples = pd.DataFrame(leaf_id).drop_duplicates().index.to_numpy()\n",
    "    \n",
    "    #Set up output rule list\n",
    "    rules = []\n",
    "    \n",
    "    #Loop over leaf node samples\n",
    "    for sample_id in samples:\n",
    "        #Create rule\n",
    "        rule = np.zeros(len(orig_features) - 1)\n",
    "        \n",
    "        #Pulls all nodes that the sample touches\n",
    "        node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                        node_indicator.indptr[sample_id + 1]]\n",
    "        \n",
    "        for node_id in node_index:\n",
    "            # continue to the next node if it is a leaf node\n",
    "            if leaf_id[sample_id] == node_id:\n",
    "                #Pull prediction for that node (majority class)\n",
    "                leaf_val = np.argmax(tree.tree_.value[node_id])\n",
    "                continue\n",
    "                \n",
    "            # check if value of the split feature for sample is below threshold\n",
    "            if (X[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "                #Get the complement of the feature\n",
    "                feat_index = orig_features.index(subset_c[feature[node_id]])\n",
    "            else:\n",
    "                #If it's 'true' get the name of the feature\n",
    "                feat_index = orig_features.index(subset[feature[node_id]])\n",
    "            \n",
    "            #Set the associated feature in thee rule ot be 1\n",
    "            rule[feat_index] = 1\n",
    "        \n",
    "        #If it's a rule for the positive class, add it\n",
    "        if leaf_val == 1:\n",
    "            rules.append(rule)\n",
    "            \n",
    "    return np.array(rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Code to train and extract rules from a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('******** FOLD: %d ********'%i)\n",
    "    for data_set in ['adult','compas','default']:\n",
    "        print('******** Dataset: %s********'%data_set)\n",
    "        train  = pd.read_csv('split_data/bin_'+data_set+'_train_%d.csv'%i)\n",
    "        orig_features = list(train.columns)\n",
    "        subset = [x for x in orig_features if ('.1' not in x) and ('*' not in x)]\n",
    "        subset_c = [x for x in orig_features if x not in subset]\n",
    "\n",
    "        train_sub = train[subset]\n",
    "        firstFlag = True\n",
    "        for depth in [2, 5, 10, 15, 20]:\n",
    "            classifier = DecisionTreeClassifier(max_depth=depth)\n",
    "            classifier.fit(train_sub.drop('Y',axis=1), train['Y'])\n",
    "            extracted_rules = extract_rules(classifier, train_sub.drop('Y',axis=1).to_numpy(), \n",
    "                                        orig_features, subset, subset_c)\n",
    "            if len(extracted_rules) == 0:\n",
    "                continue\n",
    "                \n",
    "            if firstFlag:\n",
    "                rules = extracted_rules\n",
    "                firstFlag = False\n",
    "            else:\n",
    "                rules = np.concatenate([rules, extracted_rules])\n",
    "\n",
    "        np.save('rule_mining/dt_'+data_set+'_fold_%d'%i, rules)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Code to train and extract rules from a random forset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('******** FOLD: %d ********'%i)\n",
    "    for data_set in ['adult','compas','default']:\n",
    "        print('******** Dataset: %s********'%data_set)\n",
    "        train  = pd.read_csv('split_data/bin_'+data_set+'_train_%d.csv'%i)\n",
    "        orig_features = list(train.columns)\n",
    "        subset = [x for x in orig_features if ('.1' not in x) and ('*' not in x)]\n",
    "        subset_c = [x for x in orig_features if x not in subset]\n",
    "\n",
    "        train_sub = train[subset]\n",
    "        firstFlag = True\n",
    "        for depth in [2, 5, 10, 15, 20]:\n",
    "            classifier = RandomForestClassifier(n_estimators = 10, max_depth=depth)\n",
    "            classifier.fit(train_sub.drop('Y',axis=1), train['Y'])\n",
    "            for tree in classifier.estimators_:\n",
    "                extracted_rules = extract_rules(tree, train_sub.drop('Y',axis=1).to_numpy(), \n",
    "                                        orig_features, subset, subset_c)\n",
    "                if len(extracted_rules) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                if firstFlag:\n",
    "                    rules = extracted_rules\n",
    "                    firstFlag = False\n",
    "                else:\n",
    "                    rules = np.concatenate([rules, extracted_rules])\n",
    "\n",
    "        np.save('rule_mining/rf_'+data_set+'_fold_%d'%i, rules)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fair DT\n",
    "This code chunk trains a fair decision tree classifier using the fairlearn package (specifically exponentiated gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Equalized Odds\n",
    "results = []\n",
    "datasets = ['default','adult','compas']\n",
    "protected_features = {'compas': 'race', \n",
    "                      'adult': 'gender',\n",
    "                      'default': 'X2'\n",
    "                     }\n",
    "\n",
    "epsilon_ranges = {'compas': [0, 0.05, 0.1, 0.2, 0.5],\n",
    "                      'adult': [0, 0.02, 0.05, 0.1, 0.5],\n",
    "                      'default': [0, 0.01, 0.02 , 0.05, 0.5]\n",
    "                     }\n",
    "fairness_metrics = ['EqualizedOdds', 'EqOp']\n",
    "\n",
    "for i in range(10):\n",
    "    print('***** FOLD %d ******'%i)\n",
    "    for dataset in datasets:\n",
    "        print('**** DATASET %s ******'%dataset)\n",
    "        group_var = protected_features[dataset]\n",
    "        \n",
    "        train  = pd.read_csv('split_data/bin_'+dataset+'_train_%d.csv'%i).assign(train = True)\n",
    "        test = pd.read_csv('split_data/bin_'+dataset+'_test_%d.csv'%i).assign(train = False)\n",
    "        orig_features = list(train.columns)\n",
    "        subset = [x for x in orig_features if ('.1' not in x) and ('*' not in x)]\n",
    "        subset_c = [x for x in orig_features if x not in subset]\n",
    "        firstFlag = True\n",
    "        \n",
    "        train_sub = train[subset]\n",
    "        test_sub = test[subset]\n",
    "\n",
    "        for fairMet in fairness_metrics:\n",
    "            for eps in epsilon_ranges[dataset]:\n",
    "                for hp in np.linspace(1, 21, 5):\n",
    "                    print('Epsilon: %f'%eps)\n",
    "                    \n",
    "                    if fairMet == 'EqualizedOdds':\n",
    "                        constraint = EqualizedOdds(difference_bound = eps)\n",
    "                    elif fairMet == 'EqOp':\n",
    "                        constraint = TruePositiveRateParity(difference_bound = eps)\n",
    "                    elif fairMet == 'AccDisp':\n",
    "                        constraint = ErrorRateParity(difference_bound = eps)\n",
    "                    \n",
    "                    classifier = DecisionTreeClassifier(max_depth=hp)\n",
    "                    mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    mitigator.fit(train_sub.drop('Y',axis=1), train_sub['Y'], sensitive_features=train[group_var])\n",
    "                    end_time = time.time() - start_time\n",
    "                    \n",
    "                    for tree in mitigator.predictors_:\n",
    "                        extracted_rules = extract_rules(tree, train_sub.drop('Y',axis=1).to_numpy(), \n",
    "                                                        orig_features, subset, subset_c)\n",
    "                        if len(extracted_rules) == 0:\n",
    "                            print('no rules')\n",
    "                        elif firstFlag:\n",
    "                            rules = extracted_rules\n",
    "                            firstFlag = False\n",
    "                        else:\n",
    "                            rules = np.concatenate([rules, extracted_rules])\n",
    "\n",
    "                    preds = mitigator.predict(test_sub.drop('Y',axis=1)).astype(np.bool)\n",
    "                    y = test['Y'].to_numpy()\n",
    "                    group = test[group_var].to_numpy()\n",
    "\n",
    "                    res = compute_fairness(preds, y, group)\n",
    "                    res['epsilon'] = eps\n",
    "                    res['fold'] = i\n",
    "                    res['fairnessCriteria'] = fairMet\n",
    "                    res['algo'] = 'exp_gradient_decision_tree'\n",
    "                    res['hp'] = hp\n",
    "                    res['data_set'] = dataset\n",
    "                    res['train_time'] = end_time\n",
    "                    results.append(res)\n",
    "        np.save('rule_mining/fairdt_'+data_set+'_fold_%d'%i, rules)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
